# Fill in your name, student ID, and email address in this file.
# If you are working in a team, fill out the information for both team 
# members.

# SUBMIT THE LAB ONLY ONCE (from only one partner). This file will be
# automatically parsed in order to give both team members credit for the
# lab.

# You need to fill in the EXERCISE sections describing your solutions
# for Tasks 1, 2, and 3, as well as write the corresponding code.
# If you did one or more extra credit problems, indicate which one in the
# appropriate section below (remove the # sign first). If you have any other
# information you'd like us to know, please add it at the end of the file.

# Partner 1
Name: Zach North
Student ID: 603885768
Email: zrnorth@gmail.com


# EXERCISE 1: What method you used to make your peer download and upload
#    files in parallel?  (~1-3 sentences)
The method I used to run peer download and upload in parallel was fork() and wait().
I had each loop spin off a child to handle each download / upload request, so the
parent could continue serving in the meantime. It's a fairly simple implementation
and probably not that fast, but it does the job.

# EXERCISE 2A: What conditions did you find and fix that would have
#    triggered a buffer overrun bug?  (~1-3 sentences each)
Found a filename bug -- if an inputted filename has length > FILENAMESIZ-1, it will
overrun the filename buffer and cause bad behavior. An easy fix was to use strncpy
instead of the insecure strcpy, and to check the filename size on input.

# EXERCISE 2B: What other robustness problems did you fix?  (~1-3 sentences
#    each)
I found a problem with large files -- specifically, malicious or buggy peers could send
data that is so large it would never finish in a reasonable time. I set a limit where if
the file was larger that MAX_WRITE_SIZE it would stop and try again from another peer.
Arbitrarily this limit was set to 1GB but it could be any value. For testing I set it
smaller.

A similar problem was extremely slow peers are worthless. If they send data at a very slow
rate we should drop them and try another peer. The way I checked this was a "slow counter" --
if the client sends less than a specified amount of bytes (I arbitrarily chose 50) more than
5 times in a row, we break and try a different peer.

If we connected to a large number of peers, the written data would sometimes become
too large and overrun the TASKBUFSIZ. I fixed this issue by massively increasing the size
of this buffer, to 64KB. While it causes additional memory complexity and is admittedly
dodging the problem, it solves our case and scales much higher than is probably necessary
-- we would need to connect to a ton more peers before running into a problem again.


# EXERCISE 3: Describe the attacks you implemented for "evil mode".  Why
#    are they attacks?  (~3-10 sentences each)

# Extra credit problems
#Extra credit problem: none

# Add any other information you'd like us to know below this line.
